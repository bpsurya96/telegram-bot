"""
Vision Manager Module
Handles image captioning using local BLIP model
"""

import logging
import torch
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import io
from typing import Dict

logger = logging.getLogger(__name__)

class VisionManager:
    def __init__(self, model_name="Salesforce/blip-image-captioning-base"):
        """
        Initialize vision model
        
        Args:
            model_name: Hugging Face model name
                Options: 
                - Salesforce/blip-image-captioning-base (fastest)
                - Salesforce/blip-image-captioning-large (better quality)
        """
        self.model_name = model_name
        logger.info(f"Loading vision model: {model_name}")
        
        # Determine device
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {self.device}")
        
        try:
            # Load processor and model
            self.processor = BlipProcessor.from_pretrained(model_name)
            self.model = BlipForConditionalGeneration.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("✓ Vision model loaded successfully")
        except Exception as e:
            logger.error(f"✗ Failed to load vision model: {e}")
            raise
    
    def generate_caption(self, image_bytes: bytes, max_length: int = 50) -> str:
        """
        Generate caption for an image
        
        Args:
            image_bytes: Image as bytes
            max_length: Maximum caption length
            
        Returns:
            Generated caption string
        """
        try:
            # Load image
            image = Image.open(io.BytesIO(image_bytes))
            
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # Process image
            inputs = self.processor(image, return_tensors="pt").to(self.device)
            
            # Generate caption
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    num_beams=5,
                    early_stopping=True
                )
            
            caption = self.processor.decode(outputs[0], skip_special_tokens=True)
            return caption
        
        except Exception as e:
            logger.error(f"Error generating caption: {e}")
            return "Unable to generate caption for this image."
    
    def generate_detailed_description(self, image_bytes: bytes) -> Dict:
        """
        Generate detailed description with caption and tags
        
        Args:
            image_bytes: Image as bytes
            
        Returns:
            Dictionary with caption and tags
        """
        try:
            # Generate base caption
            caption = self.generate_caption(image_bytes)
            
            # Extract potential tags from caption (simple keyword extraction)
            # For better tags, you'd use a separate model like CLIP
            words = caption.lower().split()
            
            # Simple tag extraction: nouns and adjectives
            # In production, use proper NLP or CLIP for tags
            stop_words = {'a', 'an', 'the', 'is', 'are', 'of', 'in', 'on', 'at', 'to', 'with', 'and', 'or'}
            potential_tags = [w for w in words if w not in stop_words and len(w) > 3]
            tags = potential_tags[:3] if len(potential_tags) >= 3 else potential_tags
            
            logger.info(f"Generated caption: {caption}")
            logger.info(f"Extracted tags: {tags}")
            
            return {
                'caption': caption,
                'tags': tags,
                'model_used': self.model_name
            }
        
        except Exception as e:
            logger.error(f"Error generating description: {e}", exc_info=True)
            return {
                'caption': 'Unable to process image',
                'tags': [],
                'error': str(e)
            }
    
    def analyze_image(self, image_bytes: bytes) -> str:
        """
        Full image analysis with formatted response
        
        Args:
            image_bytes: Image as bytes
            
        Returns:
            Formatted string with caption and tags
        """
        result = self.generate_detailed_description(image_bytes)
        
        tags_str = ", ".join(result['tags']) if result['tags'] else "N/A"
        
        formatted = f"""**Caption:** {result['caption']}

**Tags:** {tags_str}

_Generated by {self.model_name.split('/')[-1]}_"""
        
        return formatted