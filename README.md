# ğŸ¤– Telegram RAG Bot with Local Models

A fully local GenAI Telegram bot that uses **Ollama** for LLM, **BLIP** for image captioning, and **sentence-transformers** for embeddings. No external API calls required!

## âœ¨ Features

### ğŸ“š RAG (Retrieval-Augmented Generation)
- Semantic search over knowledge base using local embeddings
- Context-aware answers using Ollama (LLaMA 3.2, Mistral, Phi-3)
- Source attribution for transparency

### ğŸ–¼ï¸ Image Description
- Local BLIP model for image captioning
- Generate tags and descriptions
- Works offline!

### ğŸ’¬ Conversation Management
- Maintains conversation history (last 5 messages)
- `/summarize` command for conversation overview
- Context-aware responses

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Telegram   â”‚
â”‚     User     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python-telegram-bot Handler   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚
        â”‚ /ask query    â”‚ Image Upload
        â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Store  â”‚  â”‚Vision Managerâ”‚
â”‚  (ChromaDB)    â”‚  â”‚   (BLIP)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Embed query â”‚  â”‚ 1. Load imageâ”‚
â”‚    (sentence-  â”‚  â”‚              â”‚
â”‚    transformersâ”‚  â”‚ 2. Generate  â”‚
â”‚                â”‚  â”‚    caption   â”‚
â”‚ 2. Search      â”‚  â”‚              â”‚
â”‚    ChromaDB    â”‚  â”‚ 3. Extract   â”‚
â”‚                â”‚  â”‚    tags      â”‚
â”‚ 3. Retrieve    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚    top-k docs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM Manager         â”‚
â”‚      (Ollama)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Build prompt with    â”‚
â”‚    context + history    â”‚
â”‚                         â”‚
â”‚ 2. Call Ollama API      â”‚
â”‚    (local LLaMA/Mistral)â”‚
â”‚                         â”‚
â”‚ 3. Generate response    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Response         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.9+**
2. **Ollama** - [Install Ollama](https://ollama.ai)
3. **Telegram Bot Token** - Get from [@BotFather](https://t.me/botfather)

### Step 1: Install Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

### Step 2: Pull an LLM Model

```bash
# Start Ollama server
ollama serve

# In another terminal, pull a model (choose one)
ollama pull llama3.2:3b      # Fast, 2GB (Recommended)
ollama pull mistral:7b       # Balanced, 4GB
ollama pull phi3:mini        # Tiny, 1.5GB
```

### Step 3: Clone and Setup

```bash
git clone <your-repo>
cd telegram-rag-bot

# Create virtual environment
python -m venv venv
source venv/bin/activate  

# Install dependencies
pip install -r requirements.txt
```

### Step 4: Configure Environment

Create `.env` file:
```bash
cp .env.example .env
```

Edit `.env`:
```bash
# Required
TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here

# Optional (defaults shown)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
EMBEDDING_MODEL=all-MiniLM-L6-v2
VISION_MODEL=Salesforce/blip-image-captioning-base
RETRIEVAL_K=3
MAX_HISTORY_LENGTH=5
```

### Step 5: Run the Bot

```bash
python bot.py
```

First run will:
1. Download embedding model (~90MB)
2. Download BLIP model (~1GB)
3. Initialize ChromaDB with knowledge base
4. Start listening for messages

## ğŸ“– Usage Guide

### RAG Queries

```
/ask What is Python used for?
```

**Response:**
```
Python is widely used in web development (Django, Flask), 
data science (pandas, NumPy), machine learning (TensorFlow, 
PyTorch), automation, and more.

ğŸ“š Sources:
â€¢ python_basics.md
```

### Image Description

Simply send any image:

```
[Upload sunset.jpg]
```

**Response:**
```
**Caption:** a beautiful sunset over the ocean with orange and pink sky

**Tags:** sunset, ocean, beautiful

Generated by blip-image-captioning-base
```

### Other Commands

- `/stats` - Show system statistics
- `/summarize` - Summarize conversation
- `/clear` - Clear history
- `/help` - Show help

## ğŸ”§ Customization

### Add Your Own Documents

Edit `knowledge_base.py`:

```python
DOCUMENTS = [
    {
        "id": "custom_1",
        "title": "Your Document Title",
        "content": """
        Your document content here...
        Can be multiple paragraphs.
        """,
        "metadata": {"source": "your_file.md", "category": "custom"}
    },
    # Add more documents...
]
```

### Change LLM Model

```bash
# Pull different model
ollama pull mistral:7b

# Update .env
OLLAMA_MODEL=mistral:7b
```

Available models:
- `llama3.2:3b` - Fast, good quality (2GB)
- `llama3.2:1b` - Fastest, basic quality (1GB)
- `mistral:7b` - Best quality (4GB)
- `phi3:mini` - Tiny, decent quality (1.5GB)

### Change Embedding Model

```bash
# Update .env
EMBEDDING_MODEL=multi-qa-MiniLM-L6-cos-v1
```

Options:
- `all-MiniLM-L6-v2` - Fast, 384 dimensions
- `multi-qa-MiniLM-L6-cos-v1` - Better for Q&A
- `all-mpnet-base-v2` - Best quality, slower

### Change Vision Model

```bash
# Update .env
VISION_MODEL=Salesforce/blip-image-captioning-large
```

Options:
- `Salesforce/blip-image-captioning-base` - Fast (~1GB)
- `Salesforce/blip-image-captioning-large` - Better quality (~2GB)

## ğŸ³ Docker Deployment

Create `Dockerfile`:
```dockerfile
FROM python:3.11-slim

# Install Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Start Ollama and bot
CMD ollama serve & sleep 5 && ollama pull llama3.2:3b && python bot.py
```

Create `docker-compose.yml`:
```yaml
version: '3.8'
services:
  bot:
    build: .
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - OLLAMA_HOST=http://localhost:11434
    volumes:
      - ./chroma_db:/app/chroma_db
      - ollama_models:/root/.ollama
    restart: unless-stopped

volumes:
  ollama_models:
```

Run:
```bash
docker-compose up -d
```

## ğŸ“Š Model Comparison

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| **LLM Models** |
| llama3.2:1b | 1GB | âš¡âš¡âš¡ | â­â­ | Testing, low resources |
| llama3.2:3b | 2GB | âš¡âš¡ | â­â­â­ | **Recommended** |
| mistral:7b | 4GB | âš¡ | â­â­â­â­ | Best quality |
| phi3:mini | 1.5GB | âš¡âš¡ | â­â­â­ | Good balance |
| **Embedding Models** |
| all-MiniLM-L6-v2 | 90MB | âš¡âš¡âš¡ | â­â­â­ | **Recommended** |
| all-mpnet-base-v2 | 420MB | âš¡âš¡ | â­â­â­â­ | Best quality |
| **Vision Models** |
| BLIP base | 1GB | âš¡âš¡ | â­â­â­ | **Recommended** |
| BLIP large | 2GB | âš¡ | â­â­â­â­ | Better captions |

## ğŸ¯ Design Decisions

### Why Ollama?
- âœ… Easy installation and model management
- âœ… Fast inference on CPU/GPU
- âœ… Large model selection
- âœ… Compatible with OpenAI API format

### Why ChromaDB?
- âœ… Lightweight, no external server
- âœ… Fast similarity search
- âœ… Persistent storage
- âœ… Simple Python API

### Why BLIP?
- âœ… State-of-the-art image captioning
- âœ… Works on CPU (slower) or GPU (fast)
- âœ… Good balance of quality and speed
- âœ… Easy to integrate with Transformers

### Why sentence-transformers?
- âœ… No API calls, fully local
- âœ… Fast CPU inference
- âœ… High-quality embeddings
- âœ… Many pre-trained models

## ğŸ› Troubleshooting

### Ollama Connection Error
```
âœ— Failed to connect to Ollama
```

**Solution:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve

# Pull model
ollama pull llama3.2:3b
```

### Vision Model Download Slow
```
Downloading model... (this may take a while)
```

**Solution:**
- First run downloads ~1GB BLIP model
- Use `VISION_MODEL=Salesforce/blip-image-captioning-base` for faster download
- Models cached in `~/.cache/huggingface/`

### Out of Memory
```
RuntimeError: CUDA out of memory
```

**Solution:**
```bash
# Use smaller models
OLLAMA_MODEL=llama3.2:1b
VISION_MODEL=Salesforce/blip-image-captioning-base

# Or force CPU
export CUDA_VISIBLE_DEVICES=""
```

### Slow Responses
- LLM response: 3-10s on CPU (normal)
- Speed up: Use GPU or smaller model
- Consider `llama3.2:1b` for faster inference

## ğŸ“ˆ Performance Benchmarks

**Hardware:** MacBook Pro M2, 16GB RAM

| Operation | Time | Notes |
|-----------|------|-------|
| Embed query | ~50ms | CPU, all-MiniLM |
| Vector search | <10ms | 10 documents |
| LLM response | 3-5s | llama3.2:3b, CPU |
| Image caption | 2-3s | BLIP base, CPU |

**With GPU (NVIDIA RTX 3060):**
- LLM response: ~1s
- Image caption: ~500ms

## ğŸ”’ Privacy & Security

âœ… **Fully Local Processing**
- No data sent to external APIs
- All models run on your server
- Complete control over data

âš ï¸ **Considerations**
- Validate user inputs
- Rate limit commands to prevent abuse
- Don't expose bot publicly without auth
- Keep Ollama/models updated

## ğŸ“ TODO / Future Enhancements

- [ ] Add PDF/DOCX document upload
- [ ] Implement query caching
- [ ] Add web UI (Gradio/Streamlit)
- [ ] Support for audio messages
- [ ] Multi-language support
- [ ] User feedback system (ğŸ‘/ğŸ‘)
- [ ] Export conversation history
- [ ] Admin commands for monitoring

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Follow PEP 8 style guide
5. Submit a pull request

## ğŸ“„ License

MIT License - Free to use and modify!

## ğŸ™ Credits

Built with:
- [Ollama](https://ollama.ai) - Local LLM runtime
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [sentence-transformers](https://www.sbert.net/) - Embeddings
- [Transformers](https://huggingface.co/transformers) - BLIP model
- [python-telegram-bot](https://python-telegram-bot.org/) - Telegram API

---

**Need help?** Open an issue on GitHub!

**Like this project?** Give it a â­!